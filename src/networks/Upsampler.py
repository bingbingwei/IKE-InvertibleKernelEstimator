import loss
from networks import network
import torch.nn as nn
from utils.util import *


class GANupsampler:

    lambda_bicubic = 500

    def __init__(self, conf, tb_logger, task_id):
        self.conf = conf
        self.tb_logger = tb_logger
        self.task_id = task_id

        self.scale = 1 / conf.scale_factor
        self.U = network.Upsample(conf).cuda()
        self.U.apply(network.weights_init_G)

        self.UD = network.UpsampleDiscriminator(int((self.scale ** 2 - 1)), conf).cuda()
        self.UD.apply(network.weights_init_D)

        self.ud_input_shape = int(self.U.crop_size//self.scale)
        self.ud_output_shape = self.ud_input_shape - self.UD.forward_shave

        self.optimizer_U = torch.optim.Adam(self.U.parameters(), lr=conf.g_lr, betas=(conf.beta1, 0.999))
        self.optimizer_UD = torch.optim.Adam(self.UD.parameters(), lr=conf.d_lr, betas=(conf.beta1, 0.999))

        self.GAN_loss_layer = loss.PixGANLoss(d_last_layer_size=self.ud_output_shape, in_c=3).cuda()
        self.criterionGAN = self.GAN_loss_layer.forward
        self.bicubic_loss = loss.UpScaleLoss(scale_factor=self.scale).cuda()

        self.z_input = None
        self.d_input = None

        self.iter = 0

    def train_u(self):
        # Zeroize gradients
        self.optimizer_U.zero_grad()
        # Generator forward pass
        z_pred = self.U.forward(self.z_input)
        self.loss_bicubic = self.bicubic_loss.forward(g_input=self.z_input, g_output=z_pred)
        # TODO: check if upscale correct
        loss_z = 0

        for z_crop in self.crop_x2(z_pred):
            # Pass Generators output through Discriminator
            d_pred_fake = self.UD.forward(z_crop)
            # Calculate generator loss, based on discriminator prediction on generator result
            loss_z += self.criterionGAN(d_last_layer=d_pred_fake, is_d_input_real=True)
        loss_z /= len(self.crop_x2(z_pred))
        # Sum all losses
        total_loss_g = loss_z + self.loss_bicubic * self.lambda_bicubic
        # Calculate gradients
        total_loss_g.backward(retain_graph=True)
        # Update weights
        self.optimizer_U.step()

        # TensorBoard Logger add scalar
        self.tb_logger.add_scalar('Task_{}/Loss_Z'.format(self.task_id), loss_z.item(), self.iter)
        self.tb_logger.add_scalar('Task_{}/Loss_Z_bicubic'.format(self.task_id), self.loss_bicubic.item(), self.iter)

    def train_ud(self):
        self.optimizer_UD.zero_grad()
        # Discriminator forward pass over real example
        # Discriminator forward pass over fake example (generated by generator)
        # Note that generator result is detached so that gradients are not propagating back through generator
        z_output = self.U.forward(self.z_input)

        loss_d_fake = 0
        # Calculate discriminator loss
        for z_crop in self.crop_x2(z_output):
            d_pred_fake = self.UD.forward(z_crop.detach())
            loss_d_fake += self.criterionGAN(d_pred_fake, is_d_input_real=False)
        loss_d_fake /= len(self.crop_x2(z_output))

        d_pred_real = self.UD.forward(self.z_input)
        loss_d_real = self.criterionGAN(d_pred_real, is_d_input_real=True)
        loss_d = (loss_d_fake + loss_d_real) * 0.5

        # Calculate gradients, note that gradients are not propagating back through generator
        loss_d.backward(retain_graph=True)
        # Update weights, note that only discriminator weights are updated (by definition of the D optimizer)
        self.optimizer_UD.step()

        # TensorBoard Logger add scalar
        self.tb_logger.add_scalar('Task_{}/Loss_UD'.format(self.task_id), loss_d.item(), self.iter)

    def set_input(self, z_input):
        self.z_input = z_input.contiguous()

    def crop_x2(self, tensor):
        lst = []
        n, c, h, w = tensor.shape
        lst.append(tensor[:, :, :h//2, :w//2])
        lst.append(tensor[:, :, h//2:, :w//2])
        lst.append(tensor[:, :, :h//2, w//2:])
        lst.append(tensor[:, :, h//2:, w//2:])
        return lst

    def train(self, z_input):
        assert self.conf.INN_mode
        assert 'gan' in self.conf.z_upsample

        self.set_input(z_input)
        self.train_u()
        self.train_ud()
        self.tb_logger.flush()
        self.iter += 1

    def forward(self, z_input):
        return self.U.forward(z_input)

class LLRupsampler(nn.Module):

    lambda_bicubic = 5

    def __init__(self, conf, task_id, netU, tb_logger):
        super(LLRupsampler, self).__init__()
        self.conf = conf
        self.task_id = task_id

        self.scale = 1 / conf.scale_factor
        self.U = netU

        self.l1_loss = nn.L1Loss(reduction='mean')

        self.lr_input = None
        self.llr_input = None
        self.lllr_input = None
        self.z_llr = None
        self.z_lllr = None

        self.tb_logger = tb_logger
        self.iter = 0

    def get_3lr_image(self, netG):
        # get 1/4 image from original lr input
        with torch.no_grad():
            s = self.scale**2
            b, c, h, w = self.lr_input.shape
            self.lr_input = self.lr_input[:, :, :int((h//s)*s), :int((w//s)*s)]
            out = netG.forward(self.lr_input)
            self.llr_input, self.z_llr = out[:1, :, :, :], out[1:, :, :, :]

            out = netG.forward(self.llr_input)
            self.lllr_input, self.z_lllr = out[:1, :, :, :], out[1:, :, :, :]

    def cal_3lr_loss(self, netG):
        # Generator forward pass
        # TODO: input rgb into z upscale module
        z_llr_pred = self.U.forward(self.z_lllr)
        loss_u = self.l1_loss(z_llr_pred, self.z_llr)/torch.abs(torch.mean(self.z_llr))
        if (self.iter + 1) % 100 == 0:
            self.tb_logger.add_image('Peek_{}/z1 down'.format(self.task_id), tensor2tb_log(self.z_lllr[0]), self.iter)
            self.tb_logger.add_image('Peek_{}/z2 down'.format(self.task_id), tensor2tb_log(self.z_lllr[1]), self.iter)
            self.tb_logger.add_image('Peek_{}/z3 down'.format(self.task_id), tensor2tb_log(self.z_lllr[2]), self.iter)

        return loss_u

    def set_input(self, g_input):
        self.lr_input = g_input.contiguous()

    def forward(self, lr_input, netG):
        assert self.conf.INN_mode

        self.set_input(lr_input)
        self.get_3lr_image(netG)
        loss_u = self.cal_3lr_loss(netG)
        self.iter += 1
        return loss_u